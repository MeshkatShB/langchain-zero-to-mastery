{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Zero to Mastery: Part 3 - Document Processing with RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval systems and LLMs to generate answers based on external knowledge sources.\n",
    "In this part, we'll build a system that retrieves relevant documents and generates accurate answers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Installing Dependencies\n",
    "\n",
    "Ensure you have LangChain and FAISS installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: openai in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (1.58.1)\n",
      "Requirement already satisfied: faiss-cpu in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.26 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (0.3.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (0.2.6)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (2.2.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (2.10.4)\n",
      "Requirement already satisfied: requests<3,>=2 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: packaging in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: colorama in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (3.0.0)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai faiss-cpu\n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and Splitting Documents\n",
    "\n",
    "See the second example below for how to work with multiple pdf documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Documents: [{'content': 'LangChain enables seamless integration with LLMs.', 'metadata': {'title': 'LangChain Overview'}}, {'content': 'RAG systems use vector databases to retrieve relevant information.', 'metadata': {'title': 'RAG Systems'}}, {'content': 'FAISS is a library for efficient similarity search.', 'metadata': {'title': 'FAISS Library'}}]\n"
     ]
    }
   ],
   "source": [
    "# Load sample documents (you can replace this part with your own data)\n",
    "documents = [\n",
    "    {\"content\": \"LangChain enables seamless integration with LLMs.\", \"metadata\": {\"title\": \"LangChain Overview\"}},\n",
    "    {\"content\": \"RAG systems use vector databases to retrieve relevant information.\", \"metadata\": {\"title\": \"RAG Systems\"}},\n",
    "    {\"content\": \"FAISS is a library for efficient similarity search.\", \"metadata\": {\"title\": \"FAISS Library\"}},\n",
    "]\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "split_docs = []\n",
    "for doc in documents:\n",
    "    split_docs.extend(\n",
    "        [{\"content\": chunk, \"metadata\": doc[\"metadata\"]} for chunk in text_splitter.split_text(doc[\"content\"])]\n",
    "    )\n",
    "\n",
    "print(\"Split Documents:\", split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with multiple pdf files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 PDF files.\n",
      "Total split documents: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Define the directory containing your PDF files\n",
    "pdf_directory = \"./data\"\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [os.path.join(pdf_directory, f) for f in os.listdir(pdf_directory) if f.endswith(\".pdf\")]\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "\n",
    "# Load and process each PDF file\n",
    "split_docs = []\n",
    "for pdf_file in pdf_files:\n",
    "    # Load the PDF\n",
    "    pdf_loader = PyPDFLoader(pdf_file)\n",
    "    pdf_documents = pdf_loader.load()\n",
    "    \n",
    "    # Split the text into chunks and add to the list\n",
    "    for doc in pdf_documents:\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for chunk in chunks:\n",
    "            split_docs.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "\n",
    "print(f\"Processed {len(pdf_files)} PDF files.\")\n",
    "print(f\"Total split documents: {len(split_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a Vector Store\n",
    "\n",
    "See below for multiple documentation implemenation instead of the sample dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Convert dictionaries to Document objects\n",
    "documents = [\n",
    "    Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in split_docs\n",
    "]\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Build a FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Save the vector store for reuse\n",
    "vector_store.save_local(\"vector_store\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For multiple documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store saved as 'vector_store'.\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Build a FAISS vector store\n",
    "vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "# Save the vector store for reuse\n",
    "vector_store.save_local(\"vector_store_multiple_pdfs\")\n",
    "\n",
    "print(\"FAISS vector store saved as 'vector_store'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Retrieval QA System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is FAISS used for?\n",
      "Answer: FAISS is used for efficient similarity search.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define the OpenAI LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Replace \"gpt-4\" with your desired model\n",
    "\n",
    "# Define the retriever from your vector store\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Define the prompt template for combining documents\n",
    "combine_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are an assistant with access to the following context. \"\n",
    "        \"Use it to answer the question as accurately as possible.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Question:\\n{question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Build the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Specifies how documents are combined\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": combine_prompt},\n",
    ")\n",
    "\n",
    "# Query the QA chain\n",
    "query = \"What is FAISS used for?\"\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Question: {query}\\nAnswer: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For multiple documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the method of TECOMINER?\n",
      "Answer: TeCoMiner employs a method based on topological considerations regarding co-occurrence networks of terms, rather than relying on generative probabilistic models like many traditional topic modeling tools. It identifies topics as communities within these term networks. This approach allows users to explore topics interactively and provides advantages in terms of topic interpretation and control over topic granularity. The tool facilitates the discovery of semantically similar terms by mapping topic terms into a 300-dimensional vector space using a pre-trained fastText word embedding and applying agglomerative clustering based on Euclidean distance.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define the OpenAI LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Replace \"gpt-4\" with your desired model\n",
    "\n",
    "# Define the retriever from your vector store\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Define the prompt template for combining documents\n",
    "combine_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are an assistant with access to the following context. \"\n",
    "        \"Use it to answer the question as accurately as possible.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Question:\\n{question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Build the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Specifies how documents are combined\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": combine_prompt},\n",
    ")\n",
    "\n",
    "# Query the QA chain\n",
    "query = \"What is the method of TECOMINER?\"\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Question: {query}\\nAnswer: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Querying the System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is RAG?\n",
      "Answer: RAG, or Retrieval-Augmented Generation, is a system that enhances the capabilities of language models by incorporating a retrieval mechanism to access relevant information from external sources, often using vector databases. This approach allows RAG systems to generate more informed and accurate responses by retrieving contextually relevant data during the generation process.\n",
      "Question: What is FAISS used for?\n",
      "Answer: FAISS is used for efficient similarity search.\n"
     ]
    }
   ],
   "source": [
    "# First query example\n",
    "query = \"What is RAG?\"\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Question: {query}\\nAnswer: {response}\")\n",
    "\n",
    "# Another query\n",
    "query = \"What is FAISS used for?\"\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Question: {query}\\nAnswer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this part, we:\n",
    "\n",
    "- Split documents into manageable chunks.\n",
    "- Created a FAISS-based vector store for similarity search.\n",
    "- Built a Retrieval QA system using LangChain.\n",
    "\n",
    "I'm glad you're following till now! Keep up the good work! In the next part, we will explore custom agents and tool integrations for more complex applications. Stay tuned!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
