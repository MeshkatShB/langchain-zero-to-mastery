{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Zero to Mastery: Part 3 - Document Processing with RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval systems and LLMs to generate answers based on external knowledge sources.\n",
    "In this part, we'll build a system that retrieves relevant documents and generates accurate answers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Installing Dependencies\n",
    "\n",
    "Ensure you have LangChain and FAISS installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: openai in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (1.58.1)\n",
      "Requirement already satisfied: faiss-cpu in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.26 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (0.3.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (0.2.6)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (2.2.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (2.10.4)\n",
      "Requirement already satisfied: requests<3,>=2 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: packaging in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: colorama in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in h:\\documents\\work\\langchain-zero-to-mastery\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (3.0.0)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai faiss-cpu\n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and Splitting Documents\n",
    "\n",
    "See the second example below for how to work with multiple pdf documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Documents: [{'content': 'LangChain enables seamless integration with LLMs.', 'metadata': {'title': 'LangChain Overview'}}, {'content': 'RAG systems use vector databases to retrieve relevant information.', 'metadata': {'title': 'RAG Systems'}}, {'content': 'FAISS is a library for efficient similarity search.', 'metadata': {'title': 'FAISS Library'}}]\n"
     ]
    }
   ],
   "source": [
    "# Load sample documents (you can replace this part with your own data)\n",
    "documents = [\n",
    "    {\"content\": \"LangChain enables seamless integration with LLMs.\", \"metadata\": {\"title\": \"LangChain Overview\"}},\n",
    "    {\"content\": \"RAG systems use vector databases to retrieve relevant information.\", \"metadata\": {\"title\": \"RAG Systems\"}},\n",
    "    {\"content\": \"FAISS is a library for efficient similarity search.\", \"metadata\": {\"title\": \"FAISS Library\"}},\n",
    "]\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "split_docs = []\n",
    "for doc in documents:\n",
    "    split_docs.extend(\n",
    "        [{\"content\": chunk, \"metadata\": doc[\"metadata\"]} for chunk in text_splitter.split_text(doc[\"content\"])]\n",
    "    )\n",
    "\n",
    "print(\"Split Documents:\", split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with multiple pdf files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 PDF files.\n",
      "Total split documents: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Define the directory containing your PDF files\n",
    "pdf_directory = \"./data\"\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [os.path.join(pdf_directory, f) for f in os.listdir(pdf_directory) if f.endswith(\".pdf\")]\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "\n",
    "# Load and process each PDF file\n",
    "split_docs = []\n",
    "for pdf_file in pdf_files:\n",
    "    # Load the PDF\n",
    "    pdf_loader = PyPDFLoader(pdf_file)\n",
    "    pdf_documents = pdf_loader.load()\n",
    "    \n",
    "    # Split the text into chunks and add to the list\n",
    "    for doc in pdf_documents:\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for chunk in chunks:\n",
    "            split_docs.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "\n",
    "print(f\"Processed {len(pdf_files)} PDF files.\")\n",
    "print(f\"Total split documents: {len(split_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a Vector Store\n",
    "\n",
    "See below for multiple documentation implemenation instead of the sample dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Convert dictionaries to Document objects\n",
    "documents = [\n",
    "    Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in split_docs\n",
    "]\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Build a FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Save the vector store for reuse\n",
    "vector_store.save_local(\"vector_store\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For multiple documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store saved as 'vector_store'.\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Build a FAISS vector store\n",
    "vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "# Save the vector store for reuse\n",
    "vector_store.save_local(\"vector_store_multiple_pdfs\")\n",
    "\n",
    "print(\"FAISS vector store saved as 'vector_store'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Retrieval QA System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is FAISS used for?\n",
      "Answer: FAISS is used for efficient similarity search.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define the OpenAI LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Replace \"gpt-4\" with your desired model\n",
    "\n",
    "# Define the retriever from your vector store\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Define the prompt template for combining documents\n",
    "combine_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are an assistant with access to the following context. \"\n",
    "        \"Use it to answer the question as accurately as possible.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Question:\\n{question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Build the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Specifies how documents are combined\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": combine_prompt},\n",
    ")\n",
    "\n",
    "# Query the QA chain\n",
    "query = \"What is FAISS used for?\"\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Question: {query}\\nAnswer: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For multiple documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the method of TECOMINER?\n",
      "Answer: TeCoMiner employs a method based on topological considerations regarding co-occurrence networks of terms, rather than relying on generative probabilistic models like many traditional topic modeling tools. It identifies topics as communities within these term networks. This approach allows users to explore topics interactively and provides advantages in terms of topic interpretation and control over topic granularity. The tool facilitates the discovery of semantically similar terms by mapping topic terms into a 300-dimensional vector space using a pre-trained fastText word embedding and applying agglomerative clustering based on Euclidean distance.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define the OpenAI LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Replace \"gpt-4\" with your desired model\n",
    "\n",
    "# Define the retriever from your vector store\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Define the prompt template for combining documents\n",
    "combine_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are an assistant with access to the following context. \"\n",
    "        \"Use it to answer the question as accurately as possible.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Question:\\n{question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Build the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Specifies how documents are combined\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": combine_prompt},\n",
    ")\n",
    "\n",
    "# Query the QA chain\n",
    "query = \"What is the method of TECOMINER?\"\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Question: {query}\\nAnswer: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Querying the System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is RAG?\n",
      "Answer: RAG, or Retrieval-Augmented Generation, is a system that enhances the capabilities of language models by incorporating a retrieval mechanism to access relevant information from external sources, often using vector databases. This approach allows RAG systems to generate more informed and accurate responses by retrieving contextually relevant data during the generation process.\n",
      "Question: What is FAISS used for?\n",
      "Answer: FAISS is used for efficient similarity search.\n"
     ]
    }
   ],
   "source": [
    "# First query example\n",
    "query = \"What is RAG?\"\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Question: {query}\\nAnswer: {response}\")\n",
    "\n",
    "# Another query\n",
    "query = \"What is FAISS used for?\"\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Question: {query}\\nAnswer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Seamless ChatBot\n",
    "\n",
    "In this part, we use `ConversationalRetrievalChain` instead of `RetrRetrievalQA` because of the following differences:\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| **Aspect**                | **RetrievalQA**                      | **ConversationalRetrievalChain**           |\n",
    "| ------------------------- | ------------------------------------ | ------------------------------------------ |\n",
    "| **Context Handling**      | Does not handle context (stateless). | Handles context (stateful, multi-turn).    |\n",
    "| **Complexity**            | Simpler and faster.                  | More complex due to history handling.      |\n",
    "| **Use Case**              | Single-turn QA.                      | Conversational, multi-turn dialogue.       |\n",
    "| **Example Question Type** | \"What is the capital of France?\"     | \"Who is he?\" (after discussing Napoleon).  |\n",
    "| **Conversation Memory**   | Not supported.                       | Maintains and uses conversational history. |\n",
    "\n",
    "---\n",
    "\n",
    "### Which One to Choose?\n",
    "\n",
    "- **Use `RetrievalQA`** for:\n",
    "\n",
    "  - Quick, one-off queries.\n",
    "  - Tasks where conversational memory is unnecessary.\n",
    "  - Simple document search or knowledge base QA.\n",
    "\n",
    "- **Use `ConversationalRetrievalChain`** for:\n",
    "  - Chatbots or virtual assistants.\n",
    "  - Applications where user queries depend on previous interactions.\n",
    "  - Enhanced natural language understanding in dialogue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DATA_DIR path: h:\\Documents\\Work\\langchain-zero-to-mastery\\data\n",
      "Vector store not found. Creating a new one...\n",
      "load_documents function is called.\n",
      "Files in directory '../data': ['nke-10k-2023.pdf', 'TECOMINER Topic Discovery Through Term Community Detection - Mar 2021.pdf']\n",
      "Processing file: ../data\\nke-10k-2023.pdf\n",
      "Processing file: ../data\\TECOMINER Topic Discovery Through Term Community Detection - Mar 2021.pdf\n",
      "Loaded 115 documents from ../data\n",
      "Vector store created and saved at vector_store.\n",
      "PDF-based ChatBot is ready! Type your questions below. Type '/exit' to end the conversation.\n",
      "\n",
      "ChatBot: TeCoMiner is an interactive tool designed for exploring the topic content of text collections. Unlike traditional topic modeling tools that rely on generative probabilistic models, TeCoMiner is based on topological considerations of co-occurrence networks of terms. It allows users to identify topics, visualize them, and analyze their interrelations within large document datasets. The tool is particularly useful for decision-makers and analysts who want to gain insights into the topic structure of text corpora, such as scientific publications and policy-related news.\n",
      "ChatBot: The highlights of Nike's brand revenue for fiscal year 2023 are as follows:\n",
      "\n",
      "- NIKE, Inc. Revenues were $51.2 billion, which represents a 10% increase on a reported basis and a 16% increase on a currency-neutral basis compared to fiscal 2022.\n",
      "- NIKE Brand revenues, which accounted for over 90% of NIKE, Inc. Revenues, also increased by 10% on a reported basis and 16% on a currency-neutral basis.\n",
      "- The revenue growth was primarily driven by higher sales in Men's (17% growth), the Jordan Brand (35% growth), Women's (11% growth), and Kids' (10% growth) categories on a wholesale equivalent basis.\n",
      "- NIKE Direct revenues grew 14%, from $18.7 billion in fiscal 2022 to $21.3 billion in fiscal 2023, representing approximately 44% of total NIKE Brand revenues for fiscal 2023.\n",
      "- The gross margin for the fiscal year decreased by 250 basis points to 43.5%, mainly due to higher product costs, higher markdowns, and unfavorable changes in foreign currency exchange rates, although this was partially offset by strategic pricing actions.\n",
      "ChatBot: I don't know.\n",
      "ChatBot: Hello! How can I assist you today?\n",
      "ChatBot: Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "# Path to data and vector store\n",
    "DATA_DIR = \"../data\"\n",
    "VECTOR_STORE_PATH = \"vector_store\"\n",
    "\n",
    "# Debug: Check DATA_DIR\n",
    "print(f\"Absolute DATA_DIR path: {os.path.abspath(DATA_DIR)}\")\n",
    "\n",
    "# Function to load documents\n",
    "def load_documents(data_dir):\n",
    "    print(\"load_documents function is called.\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Error: The directory {data_dir} does not exist.\")\n",
    "        return []\n",
    "    \n",
    "    file_names = os.listdir(data_dir)\n",
    "    print(f\"Files in directory '{data_dir}': {file_names}\")\n",
    "\n",
    "    documents = []\n",
    "    for file_name in file_names:\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(data_dir, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            try:\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                docs = loader.load()\n",
    "                documents.extend(docs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents from {data_dir}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "# Function to create vector store if it doesn't exist\n",
    "def create_vector_store(documents, vector_store_path):\n",
    "    # Split documents into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "    vector_store.save_local(vector_store_path)\n",
    "    print(f\"Vector store created and saved at {vector_store_path}.\")\n",
    "    return vector_store\n",
    "\n",
    "# Debug: Force vector store creation\n",
    "print(\"Vector store not found. Creating a new one...\")\n",
    "documents = load_documents(DATA_DIR)\n",
    "if not documents:\n",
    "    print(\"No documents were loaded.\")\n",
    "else:\n",
    "    vector_store = create_vector_store(documents, VECTOR_STORE_PATH)\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "# Create the ConversationalRetrievalChain\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Conversational loop\n",
    "print(\"PDF-based ChatBot is ready! Type your questions below. Type '/exit' to end the conversation.\\n\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"/exit\":\n",
    "        print(\"ChatBot: Goodbye! Have a great day!\")\n",
    "        break\n",
    "\n",
    "    query = {\"question\": user_input}\n",
    "    response = conversational_chain.run(query)\n",
    "    print(f\"ChatBot: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this part, we:\n",
    "\n",
    "- Split documents into manageable chunks.\n",
    "- Created a FAISS-based vector store for similarity search.\n",
    "- Built a Retrieval QA system using LangChain.\n",
    "- Built a ChatBot using ConversationalRetrievalChain using LangChain.\n",
    "\n",
    "I'm glad you're following till now! Keep up the good work! In the next part, we will explore custom agents and tool integrations for more complex applications. Stay tuned!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
